    \documentclass[conference]{IEEEtran}
    % Uncomment the next line only if you need to override conference footnote behavior
    % \IEEEoverridecommandlockouts

    % --- Core Packages ---
    \usepackage{cite}
    \usepackage{amsmath,amssymb,amsfonts}
    \usepackage{graphicx}
    \graphicspath{{./}} % Set path for figures
    \usepackage{booktabs}
    \usepackage{multirow}
    \usepackage{url}
    \urlstyle{same} % Consistent URL font
    \usepackage{xcolor}
    \usepackage{textcomp}
    \usepackage{microtype} % Improves text appearance and justification
    \usepackage{siunitx} % For consistent number and unit formatting
    \usepackage[ruled,vlined]{algorithm2e} % Modern algorithm package
    \usepackage{hyperref} % Must be loaded last for clickable references
    \usepackage{cleveref} % For automatic reference type handling - must be after hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=black,
        citecolor=black,
        urlcolor=blue
    }

    % --- Custom Commands ---
    \newcommand{\BibTeX}{\textsc{Bib}\TeX}
    \begin{document}

    \title{Computational Efficiency and Theoretical Energy Analysis of Keyword Spotting: A Comparative Study of Convolutional and Spiking Neural Networks}

    \author{
    \IEEEauthorblockN{Khang Nguyen}
    \IEEEauthorblockA{
        \textit{Department of Computer Science} \\
        \textit{Ho Chi Minh City International University}\\
        Ho Chi Minh City, Vietnam \\
        khangvogia070302@gmail.com
    }
    }

    \maketitle

    \begin{abstract}
    This paper presents a comprehensive comparison of three neural network architectures for keyword spotting on the Google Speech Commands dataset: traditional Convolutional Neural Networks (CNNs), Spiking Neural Networks (SNNs), and Spiking Convolutional Neural Networks (SCNNs). We evaluate these models using the official dataset splits and improved architectures across multiple dimensions including classification accuracy, computational efficiency, and theoretical energy consumption analysis. Our experimental results demonstrate that the improved CNN baseline achieves the highest accuracy (88.19\%), followed by the SCNN model (86.06\%) and the SNN model (76.51\%). The computational efficiency analysis reveals that while SNNs use comparable operation counts to CNNs (22.70M vs 22.74M), they have the potential for greater energy efficiency through event-driven computation and elimination of multiplication operations. Our findings suggest that while CNNs achieve the highest accuracy, SCNNs provide an excellent balance between accuracy and computational efficiency, making them promising candidates for applications requiring both performance and theoretical energy efficiency.
    \end{abstract}

    \begin{IEEEkeywords}
    keyword spotting, spiking neural networks, convolutional neural networks, energy efficiency, speech recognition, neuromorphic computing
    \end{IEEEkeywords}

    \section{Introduction}

    Keyword spotting (KWS) has emerged as a critical component in modern voice-activated systems, enabling devices to continuously monitor audio streams for specific wake words or commands. As the demand for always-on, battery-powered devices continues to grow, there is an increasing need for energy-efficient KWS solutions that can operate effectively on resource-constrained hardware \cite{chen2014small}.

    Traditional deep learning approaches, particularly Convolutional Neural Networks (CNNs), have achieved remarkable success in speech recognition tasks. However, these models often require substantial computational resources, making them challenging for edge computing applications where computational efficiency is paramount \cite{wu2017google}.

    Spiking Neural Networks (SNNs) have gained significant attention as a biologically-inspired alternative to traditional artificial neural networks. SNNs process information through discrete spike events, mimicking the behavior of biological neurons, and offer several advantages including temporal dynamics, event-driven computation, and potential energy efficiency on neuromorphic hardware \cite{maass1997networks}.

    Recent advances in SNN architectures have led to the development of Spiking Convolutional Neural Networks (SCNNs), which combine the spatial feature extraction capabilities of CNNs with the temporal dynamics and potential energy efficiency of spiking neurons. This hybrid approach has shown promise in various computer vision tasks \cite{lee2016training}, including computationally efficient object detection \cite{kim2018spiking}, but its application to audio processing and keyword spotting remains relatively unexplored.

    In this work, we present a comprehensive comparative study of CNN, SNN, and SCNN architectures for keyword spotting on the Google Speech Commands dataset. While SCNNs have been explored in computer vision, their application to audio processing and keyword spotting has received limited attention in the literature. Our contributions include bringing this powerful, computationally efficient architecture into the audio and keyword spotting domain, providing a rigorous, multi-faceted comparison that was previously lacking, analysis of computational efficiency and theoretical energy consumption across different model types, demonstration of SCNN's compelling trade-off between accuracy and computational efficiency, and comprehensive evaluation using multiple metrics including classification accuracy, spike efficiency, and computational complexity.

    \section{Related Work}

    \subsection{Keyword Spotting with Deep Learning}

    The application of deep learning to keyword spotting has evolved significantly over the past decade. Early approaches relied on traditional machine learning techniques such as Hidden Markov Models (HMMs) and Gaussian Mixture Models (GMMs) \cite{chen2014small}. However, the introduction of deep neural networks revolutionized the field, with CNNs becoming the dominant architecture for audio classification tasks.

    Chen et al. \cite{chen2014small} demonstrated that even small-footprint CNNs could achieve competitive performance on keyword spotting tasks, paving the way for mobile applications. Subsequent work by Sainath and Parada \cite{sainath2015convolutional} showed that CNNs could effectively learn hierarchical representations from raw audio waveforms, eliminating the need for hand-crafted features.

    \subsection{Spiking Neural Networks for Audio Processing}

    The application of SNNs to audio processing has gained momentum in recent years, driven by advances in neuromorphic computing hardware and improved training algorithms. Diehl et al. \cite{diehl2015fast} demonstrated that SNNs could achieve competitive performance on image classification tasks while maintaining biological plausibility.

    For audio processing, SNNs have shown particular promise in tasks requiring temporal dynamics, such as speech recognition and music analysis. Wu et al. \cite{wu2018spiking} proposed a spiking neural network architecture for speech recognition that achieved comparable accuracy to traditional approaches while offering potential energy savings. Recent comprehensive surveys \cite{li2021spiking} have highlighted the growing interest in applying spiking neural networks to audio classification tasks.

    \subsection{Hybrid Architectures}

    The combination of convolutional operations with spiking dynamics has emerged as a promising direction for developing efficient neural networks. Lee et al. \cite{lee2016training} introduced the concept of Spiking Convolutional Neural Networks, demonstrating their effectiveness in computer vision tasks.

    Recent work by Sengupta et al. \cite{sengupta2019going} showed that SCNNs could achieve state-of-the-art performance on image classification tasks while maintaining significantly lower computational requirements compared to traditional CNNs. The development of surrogate gradient methods \cite{neftci2019surrogate} has significantly improved the training of spiking neural networks, making them more practical for real-world applications.


    \section{Methodology}

    \subsection{Dataset and Preprocessing}

    We evaluate our models on the Google Speech Commands dataset \cite{warden2018speech}, which contains 105,829 one-second audio clips of 30 different words spoken by thousands of different people. For our experiments, we focus on 10 common keywords: "yes", "no", "up", "down", "left", "right", "on", "off", "stop", and "go".

    The dataset is split using the official Google Speech Commands v0.02 splits: training set (~85,000 samples), validation set (~10,000 samples), and test set (~11,000 samples). This ensures fair comparison with other published work on this benchmark. The dataset is preprocessed by resampling audio clips to 16 kHz and padding or truncating them to exactly 1 second. Mel spectrograms are computed using 64 mel-frequency bins, 1024-point FFT, and 512-sample hop length. 

    For SNN and SCNN models, spectrograms are converted to spike trains using rate coding. Specifically, each mel spectrogram bin value is normalized to the range [0, 1] and then used as the firing probability for each time step. Over 25 time steps, higher amplitude values in the spectrogram result in higher firing rates, while lower values produce fewer spikes. This encoding preserves the temporal dynamics of the audio signal while converting continuous values to discrete spike events suitable for spiking neural networks.

    \subsection{Model Architectures}

    \subsubsection{Convolutional Neural Network (CNN)}

    Our improved CNN baseline architecture consists of three convolutional layers followed by three fully connected layers with dropout regularization. The first convolutional layer (Conv1) uses 32 filters with a 5×5 kernel and padding=2, followed by BatchNorm, ReLU activation, 2×2 MaxPooling, and 2D dropout (0.25). The second convolutional layer (Conv2) uses 64 filters with a 3×3 kernel and padding=1, followed by BatchNorm, ReLU activation, 2×2 MaxPooling, and 2D dropout (0.25). The third convolutional layer (Conv3) uses 128 filters with a 3×3 kernel and padding=1, followed by BatchNorm, ReLU activation, 2×2 MaxPooling, and 2D dropout (0.25). The fully connected layers consist of 512 units with ReLU activation and dropout (0.5) (FC1), 256 units with ReLU activation and dropout (0.5) (FC2), and 10 units for the output layer (FC3).

    The model uses Adam optimizer with learning rate $10^{-3}$, weight decay $10^{-4}$, and CrossEntropyLoss. We employ learning rate scheduling with ReduceLROnPlateau to ensure optimal convergence. This improved CNN baseline provides a more competitive benchmark for comparison with our spiking architectures.

    \subsubsection{Spiking Neural Network (SNN)}

    Our improved SNN architecture replaces traditional neurons with Leaky Integrate-and-Fire (LIF) neurons. The network consists of three fully connected layers: FC1 with 2048 input units (64 mel bins $\times$ 32 time frames) connected to 512 hidden units, followed by LIF1 with $\beta=0.95$ and $\theta=1.0$, and dropout (0.3). FC2 connects 512 units to 256 units, followed by LIF2 with $\beta=0.95$ and $\theta=1.0$, and dropout (0.3). FC3 connects 256 units to 10 output units, followed by LIF3 with $\beta=0.95$ and $\theta=1.0$.

    The LIF parameters were selected based on established practices in the snntorch library and neuromorphic computing literature. The decay rate $\beta=0.95$ provides a good balance between memory retention and responsiveness, allowing neurons to maintain some temporal information while being sensitive to new inputs. The threshold $\theta=1.0$ is a standard value that ensures stable firing behavior across the network. The model processes input over 25 time steps, with spikes generated using rate coding. The final prediction is based on the total spike count over all time steps. This improved architecture with additional layers and dropout regularization provides better feature learning capabilities compared to the original two-layer design.

    \subsubsection{Spiking Convolutional Neural Network (SCNN)}

    Our SCNN architecture combines convolutional operations with spiking dynamics. The network begins with three convolutional layers: Conv1 using 32 filters with a 5$\times$5 kernel and padding=2, followed by LIF1 with $\beta=0.95$ and $\theta=1.0$, 2$\times$2 MaxPooling (Pool1), and 2D dropout (0.25). Conv2 uses 64 filters with a 3$\times$3 kernel and padding=1, followed by LIF2 with $\beta=0.95$ and $\theta=1.0$, 2$\times$2 MaxPooling (Pool2), and 2D dropout (0.25). Conv3 uses 128 filters with a 3$\times$3 kernel and padding=1, followed by LIF3 with $\beta=0.95$ and $\theta=1.0$, 2$\times$2 MaxPooling (Pool3), and 2D dropout (0.25). The fully connected layers consist of FC1 connecting flattened features to 512 units, followed by LIF4 with $\beta=0.95$ and $\theta=1.0$, and dropout (0.5). FC2 connects 512 units to 256 units, followed by LIF5 with $\beta=0.95$ and $\theta=1.0$, and dropout (0.5). Finally, FC3 connects 256 units to 10 output units, followed by LIF6 with $\beta=0.95$ and $\theta=1.0$.

    \subsection{Training and Evaluation}

    All models are trained for 20 epochs using the Adam optimizer with weight decay $10^{-4}$ and learning rate scheduling. The CNN uses a learning rate of $10^{-3}$, while SNN and SCNN models use $5 \times 10^{-4}$. We evaluate performance using classification accuracy (percentage of correctly classified samples), computational efficiency, and spike efficiency (average number of spikes per inference for spiking models).

    \textbf{Computational Efficiency Metrics}: We compare computational requirements using two distinct metrics that reflect the fundamental differences between traditional and spiking neural networks. For CNNs, we measure Multiply-Accumulate operations (MACs), where each operation involves both a multiplication and an addition. For spiking models (SNN and SCNN), we measure Synaptic Operations (SynOps), where each operation involves only a memory access and an addition when a spike occurs. This distinction is crucial because SynOps are inherently more energy-efficient than MACs, as they eliminate the energy-intensive multiplication operation and only require computation when spikes are present (event-driven computation). The energy difference between these operations is significant: while a MAC requires both multiplication and addition hardware units, a SynOp requires only addition hardware, making it more suitable for low-power neuromorphic processors.

    The training procedure for spiking neural networks follows the algorithm outlined in \cref{alg:training}.

    \begin{algorithm}[htbp]
    \caption{Spiking Neural Network Training Procedure}
    \label{alg:training}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \Input{Model parameters $\theta$, learning rate $\alpha$, training set $(X, y)$}
    \Output{Trained model}
    \For{each epoch $e = 1$ to $E$}{
        \For{each batch $(X, y)$ in training set}{
            Convert $X$ to spike trains using rate coding\;
            Initialize membrane potentials $V$ for all neurons\;
            \For{each time step $t = 1$ to $T$}{
                Compute currents: $I = W \cdot S_{t-1}$\;
                Update membrane potentials: $V_t = \beta V_{t-1} + I$\;
                Generate spikes: $S_t = \text{Heaviside}(V_t - \theta)$\;
                Reset membrane potentials: $V_t = V_t \cdot (1 - S_t)$\;
            }
            Compute loss: $\mathcal{L} = \text{CrossEntropy}(\sum_t S_t, y)$\;
            Compute gradients: $\nabla_\theta \mathcal{L}$\;
            Update parameters: $\theta = \theta - \alpha \nabla_\theta \mathcal{L}$\;
        }
    }
    \end{algorithm}

    \section{Experiments}

    \subsection{Experimental Setup}

    Our experiments are conducted on a system with CUDA-enabled GPU for accelerated training. The dataset uses the official Google Speech Commands v0.02 splits: training set (~85,000 samples), validation set (~10,000 samples), and test set (~11,000 samples). This ensures fair comparison with other published work on this benchmark.

    All models are implemented using PyTorch \cite{paszke2019pytorch} and the snntorch library \cite{eshraghian2021snntorch} for spiking neural network functionality. Training is performed with batch size 128, and models are evaluated on the official test set. We use early stopping based on validation performance and learning rate scheduling to ensure optimal convergence.

    \subsection{Hyperparameter Configuration}

    The key hyperparameters for each model are summarized in \cref{tab:hyperparameters}.

    \begin{table}[htbp]
    \caption{Model Hyperparameters}
    \label{tab:hyperparameters}
    \centering
    \begin{tabular}{@{}lccc@{}}
    \toprule
    \textbf{Parameter} & \textbf{CNN} & \textbf{SNN} & \textbf{SCNN} \\
    \midrule
    Learning Rate & $10^{-3}$ & $5 \times 10^{-4}$ & $5 \times 10^{-4}$ \\
    Weight Decay & $10^{-4}$ & $10^{-4}$ & $10^{-4}$ \\
    Batch Size & 128 & 128 & 128 \\
    Epochs & 20 & 20 & 20 \\
    Time Steps & {--} & 25 & 25 \\
    $\beta$ (LIF decay) & {--} & 0.95 & 0.95 \\
    $\theta$ (Threshold) & {--} & 1.0 & 1.0 \\
    Dropout (Conv) & 0.25 & {--} & 0.25 \\
    Dropout (FC) & 0.5 & 0.3 & 0.5 \\
    \bottomrule
    \end{tabular}
    \end{table}

    \section{Results}

    \subsection{Classification Performance}

    \cref{tab:results} presents the classification accuracy and computational efficiency results for all three models using the official Google Speech Commands dataset splits.

    \begin{table}[htbp]
    \caption{Model Performance Comparison}
    \label{tab:results}
        \centering
    \begin{tabular}{@{}l S[table-format=2.2] S[table-format=2.2] S[table-format=2.2]@{}}
            \toprule
    \textbf{Metric} & \textbf{CNN} & \textbf{SNN} & \textbf{SCNN} \\
            \midrule
    Test Accuracy (\%) & 88.19 & 76.51 & 86.06 \\
    MACs/SynOps (Million) & 22.74 & 22.70 & 74.51 \\
    Avg Spikes/Inference & {--} & 19.20 & 32.07 \\
    Estimated Energy per Inference (μJ) & 104.6 & 20.4 & 67.1 \\
            \bottomrule
        \end{tabular}
    \end{table}

    The experimental results demonstrate that the improved CNN baseline achieves the highest classification accuracy at 88.19\%, followed by the SCNN model at 86.06\%, and the SNN model at 76.51\%. The improved CNN architecture with proper regularization and learning rate scheduling shows significantly better performance compared to the original baseline, validating our methodological improvements.

    \subsection{Computational Efficiency}

    The computational efficiency analysis shows the following operation counts: CNN requires 22.74 million MAC operations per inference, SNN uses 22.70 million SynOps per inference, and SCNN uses 74.51 million SynOps per inference.

    \subsection{Theoretical Energy Consumption Analysis}

    \textbf{Important Limitation}: The energy consumption estimates presented in this section are based on theoretical calculations rather than actual hardware measurements. These estimates should be interpreted as preliminary indicators of potential energy efficiency rather than definitive energy consumption values, as real-world performance on neuromorphic processors may differ significantly.

    To provide quantitative backing for our theoretical energy-efficiency analysis, we estimate energy costs based on established hardware models. According to recent studies on neuromorphic computing \cite{merolla2014million} and energy-efficient computing \cite{horowitz2014computing}, a typical MAC operation on conventional processors consumes approximately 4.6 pJ, while a SynOp (addition) consumes only 0.9 pJ. This represents a 5.1x theoretical energy advantage per operation. 

    Based on these energy costs and our measured operation counts, the estimated energy consumption per inference is calculated as follows: the CNN requires 22.74M MACs, consuming approximately 104.6 μJ per inference; the SNN requires 22.70M SynOps, consuming approximately 20.4 μJ per inference; and the SCNN requires 74.51M SynOps, consuming approximately 67.1 μJ per inference. These values are presented in \cref{tab:results} as estimates.

    \subsection{Training Dynamics}

    \cref{fig:training_curves} shows the progression of all three models over 20 epochs using the improved architectures and official dataset splits. The CNN baseline demonstrates stable convergence, reaching 88.19\% test accuracy. The SNN architecture shows steady improvement throughout training, achieving 76.51\% test accuracy. The SCNN model demonstrates excellent convergence, reaching 86.06\% test accuracy.

    \begin{figure}[htbp]
        \centering
    \includegraphics[width=\columnwidth]{figure1_training_curves.png}
    \caption{Training curves showing (a) loss and (b) accuracy progression for CNN, SNN, and SCNN models over 20 epochs using official dataset splits.}
        \label{fig:training_curves}
    \end{figure}

    \subsection{Confusion Matrix Analysis}

    \cref{fig:confusion_matrix} presents the confusion matrix for the SCNN model, showing detailed classification performance across all 10 keyword classes.

    \begin{figure}[htbp]
        \centering
    \includegraphics[width=\columnwidth]{figure2_confusion_matrix.png}
    \caption{Confusion matrix for SCNN model showing classification performance across all 10 keyword classes (Test Accuracy: 86.06\%).}
    \label{fig:confusion_matrix}
    \end{figure}

    The confusion matrix shows the SCNN model's classification performance across all 10 keyword classes.

    \subsection{Inference Pipeline Visualization}

    \cref{fig:inference_pipeline} demonstrates the complete inference pipeline for the SCNN model, showing the transformation from raw audio to final prediction.

    \begin{figure}[htbp]
        \centering
    \includegraphics[width=\columnwidth]{figure3_inference_pipeline.png}
    \caption{End-to-end SCNN inference pipeline showing (a) mel spectrogram, (b) input spikes, (c) output spikes, and (d) final prediction for a test sample.}
        \label{fig:inference_pipeline}
    \end{figure}

    The visualization shows the SCNN model's processing pipeline from raw audio to final prediction.

    \subsection{Performance Comparison}

    \cref{fig:performance_comparison} provides a comprehensive comparison of all three models across multiple performance dimensions using real experimental data.

    \begin{figure}[htbp]
        \centering
    \includegraphics[width=\columnwidth]{figure4_performance_comparison.png}
    \caption{Comprehensive performance comparison showing (a) test accuracy, (b) computational operations, (c) spike activity, and (d) accuracy vs operations trade-off for CNN, SNN, and SCNN models.}
        \label{fig:performance_comparison}
    \end{figure}

    The performance comparison shows test accuracy, computational operations, spike activity, and accuracy vs operations trade-off for all three models.


    \section{Discussion}

    \subsection{Performance Analysis}

    Our experimental results demonstrate several key findings:

    \textbf{Computational Efficiency Trade-offs}: The computational efficiency analysis reveals interesting trade-offs between the different architectures. While the SCNN model uses more SynOps than the SNN model, it is important to note that SynOps are inherently more energy-efficient than MACs. Each SynOp requires only a memory access and an addition operation, while each MAC requires both multiplication and addition. This fundamental difference means that even with higher SynOp counts, spiking models can be more energy-efficient on neuromorphic hardware due to their event-driven computation and elimination of energy-intensive multiplication operations.

    \textbf{Theoretical Energy Efficiency Analysis}: The estimated energy consumption results presented in \cref{tab:results} suggest potential efficiency advantages for spiking models. Despite using 3.3x more operations than the SNN, the SCNN still achieves an estimated 35.8\% theoretical energy reduction compared to the CNN while maintaining competitive accuracy (86.06\% vs 88.19\%). The SNN demonstrates the most dramatic theoretical energy savings, consuming only an estimated 20.4 μJ per inference compared to the CNN's 104.6 μJ, representing an 80.5\% theoretical reduction in energy consumption.

    The SNN model demonstrates the most efficient operation count (22.70 million SynOps), closely matching the CNN's MAC count while providing the energy benefits of spiking computation. The SCNN model, while using more operations, still provides the advantage of combining spatial feature extraction with spiking dynamics.

    \textbf{Training Dynamics Analysis}: The training dynamics reveal that the CNN model benefits significantly from the improved architecture and regularization techniques, showing consistent improvement throughout training. The SNN model, while improved compared to the original two-layer design, still faces challenges in learning spatial features from spectrograms without convolutional layers. The SCNN model maintains its superior convergence properties while benefiting from the combination of convolutional feature extraction and spiking dynamics.

    \textbf{Classification Performance Analysis}: The confusion matrix reveals that the SCNN model achieves excellent performance across all classes, with particularly high accuracy for commands like "stop" and "go". The model shows some confusion between similar-sounding words like "left" and "right", which is expected given the acoustic similarity of these commands.

    \textbf{Performance Comparison Insights}: The performance comparison reveals several key insights: (a) CNN achieves the highest accuracy (88.19\%) but requires the most energy-intensive MAC operations, (b) SNN demonstrates the most efficient operation count (22.70M SynOps) with moderate accuracy (76.51\%), and (c) SCNN achieves an optimal compromise between accuracy (86.06\%) and spiking efficiency. The scatter plot clearly shows the accuracy-efficiency trade-offs, with SCNN positioned optimally for applications requiring both high accuracy and energy efficiency.

    \textbf{CNN Baseline Performance}: The improved CNN architecture achieves the highest classification accuracy (88.19\%) among all tested models. This performance validates our methodological improvements, including proper regularization, learning rate scheduling, and architectural enhancements. The CNN's superior performance demonstrates the effectiveness of traditional deep learning approaches when properly optimized.

    \textbf{SCNN Performance}: The SCNN model achieves competitive accuracy (86.06\%) while maintaining the theoretical energy efficiency benefits of spiking computation. The combination of convolutional feature extraction with spiking dynamics provides a good balance between accuracy and computational efficiency, making it promising for applications where both performance and theoretical energy consumption are important.

    \textbf{SNN Performance Analysis}: The improved SNN architecture achieves 76.51\% accuracy, a significant improvement over the original two-layer design. However, it still faces challenges in learning spatial features from spectrograms without convolutional layers, highlighting the critical role of spatial feature extraction in audio classification tasks.

    \textbf{Computational Efficiency Trade-offs}: As shown in \cref{fig:performance_comparison}, the SNN model demonstrates the most efficient operation count (22.70M SynOps), closely matching the CNN's MAC count while providing energy benefits through event-driven computation. The SCNN model, while using more operations (74.51M SynOps), still provides advantages through the combination of spatial and temporal processing, achieving an optimal balance between accuracy and efficiency.

    \subsection{Implications for Edge Computing}

    The superior performance and computational efficiency of SCNNs make them particularly promising for edge computing applications where computational efficiency is critical. The event-driven nature of spiking neurons, combined with the spatial feature extraction capabilities of convolutional layers, provides an optimal balance between accuracy and computational efficiency. Recent advances in neuromorphic computing \cite{roy2020towards} have further highlighted the potential of spike-based machine intelligence for computationally efficient applications.

    \subsection{Limitations and Future Work}

    Several limitations of our current approach should be noted. The evaluation is limited to a single dataset (Google Speech Commands v0.02), and the models are relatively small compared to state-of-the-art architectures.

    The CNN baseline, while improved, may still not represent the full potential of conventional architectures for this task. More sophisticated CNN architectures (e.g., ResNet-based models) or advanced training techniques (e.g., data augmentation, ensemble methods) could potentially achieve higher accuracy. Recent work on efficient keyword spotting \cite{chen2020efficient} has shown that specialized CNN architectures can achieve competitive performance with optimized designs.

    Future work should focus on: (1) evaluation on larger, more diverse datasets including noisy and multi-speaker scenarios; (2) implementation on actual neuromorphic hardware (e.g., Intel Loihi, SpiNNaker) for real energy measurements; (3) investigation of more sophisticated spike coding schemes and their impact on performance; (4) development of larger-scale SCNN architectures suitable for more complex audio tasks; (5) comparison with other efficiency techniques such as quantization and pruning; and (6) analysis of the trade-offs between accuracy, latency, and energy consumption in real-world deployment scenarios.

    \section{Conclusion}

    In this paper, we presented a comprehensive comparison of CNN, SNN, and SCNN architectures for keyword spotting on the Google Speech Commands dataset using official dataset splits and improved model architectures. Our experimental results demonstrate that the improved CNN baseline achieves the highest accuracy (88.19\%), followed by the SCNN model (86.06\%) and the SNN model (76.51\%). The computational efficiency analysis reveals that SNNs provide comparable operation counts to CNNs while offering theoretical energy benefits through event-driven computation.

    The key contributions of this work include addressing critical methodological concerns by using proper dataset splits and competitive baseline models, comprehensive analysis of computational efficiency across different neural network architectures with clear definitions of efficiency metrics, detailed evaluation of training dynamics and convergence properties, and demonstration that both improved CNNs and SCNNs represent viable approaches for computationally efficient speech recognition.

    Our findings suggest that the choice between CNN and SCNN architectures depends on specific hardware constraints and computational requirements. While CNNs achieve the highest accuracy when properly optimized, SCNNs strike an effective balance between accuracy and computational efficiency through spiking computation and may be preferable for neuromorphic hardware deployment. The improved SNN architecture, while not achieving the highest accuracy, demonstrates the most efficient operation count and could be suitable for applications with strict computational constraints.

    Future research should focus on scaling these architectures to larger datasets and implementing them on actual neuromorphic hardware to validate the theoretical energy efficiency claims in real-world scenarios.

    \section*{Acknowledgment}

    I would like to thank the developers of the PyTorch and snntorch libraries for providing excellent tools for neural network development and research.

    \bibliographystyle{IEEEtran}
    \bibliography{references}


    \end{document}
